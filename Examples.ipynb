{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This notebook is made to show, works LangTokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mult-tokenizer in c:\\users\\oksan\\anaconda3\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in c:\\users\\oksan\\anaconda3\\lib\\site-packages (from mult-tokenizer) (4.64.0)\n",
      "Requirement already satisfied: numpy>=1.21.5 in c:\\users\\oksan\\anaconda3\\lib\\site-packages (from mult-tokenizer) (1.22.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\oksan\\anaconda3\\lib\\site-packages (from tqdm>=4.64.0->mult-tokenizer) (0.4.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade mult-tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oksan\\anaconda3\\lib\\site-packages\\mg_tok\\tokenizer.py:2: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import codecs\n",
    "from mg_tok import LangTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import file with russian sentences and their translations to English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set of symbols in Russian sentences\n",
      "{'h', '6', 'з', '0', 'э', '1', 'ы', 'ё', 'ь', '«', '\"', 'р', ':', '!', 'j', 'т', '3', 'c', 'w', 'q', ',', 'г', '+', '—', 'д', 'м', 'o', 'e', 'd', '8', 's', 'а', 'о', 'п', 'ж', '»', '%', 'ш', 'я', 'ъ', 'н', 'r', 'у', 'й', 'p', 'i', 'a', 'б', '́', '4', '–', '2', 'u', 'х', 'к', '?', \"'\", 'в', 'b', 'ф', 'f', '9', 'ю', 'n', '-', '7', 'и', ' ', '.', 'л', '5', 'g', 'ц', 'k', 't', ';', 'l', 'ч', 'с', 'y', 'v', 'x', 'е', '№', 'щ', 'm'}\n",
      "86\n",
      "\n",
      "---\n",
      "\n",
      "Set of symbols in English sentences\n",
      "{'h', '6', 'r', '0', 'z', 'p', 'i', 'a', '1', '4', '2', 'u', '\"', '?', \"'\", ':', '!', 'j', 'b', '€', '3', 'w', 'c', 'f', '9', '‘', 'n', '/', '-', 'q', 'º', '7', ' ', '.', ',', '+', '5', 'g', 'o', '’', 'e', 'k', 'd', '8', 's', 't', 'ã', ';', 'é', 'l', 'y', 'v', 'x', '$', '%', 'ü', 'm'}\n",
      "57\n"
     ]
    }
   ],
   "source": [
    "rus_list_lines = []\n",
    "rus_lines = []\n",
    "eng_list_lines = []\n",
    "eng_lines = []\n",
    "rus_bad_symb = ['\\xa0', '\\u200b']\n",
    "eng_bad_symb = []\n",
    "with codecs.open('rus-eng.txt', encoding='utf-8') as fin:\n",
    "    for line in fin:\n",
    "        rus_curr = line[:-2].lower().split(\"_\")[0]\n",
    "        rus_curr_list = list(rus_curr)\n",
    "        eng_curr = line[:-2].lower().split(\"_\")[1]\n",
    "        eng_curr_list = list(eng_curr)\n",
    "        \n",
    "        for symb in rus_bad_symb:\n",
    "            while symb in rus_curr_list:\n",
    "                rus_curr_list.remove(symb)\n",
    "        rus_lines.append(\"\".join(rus_curr_list))\n",
    "        rus_list_lines.append(rus_curr_list)\n",
    "        \n",
    "        \n",
    "        for symb in eng_bad_symb:\n",
    "            while symb in eng_curr_list:\n",
    "                eng_curr_list.remove(symb)\n",
    "        eng_lines.append(\"\".join(eng_curr_list))\n",
    "        eng_list_lines.append(eng_curr_list)\n",
    "        \n",
    "rus_vocab = set([])\n",
    "eng_vocab = set([])\n",
    "\n",
    "for curr_list in rus_list_lines:\n",
    "    rus_vocab = rus_vocab.union(curr_list)\n",
    "    \n",
    "for curr_list in eng_list_lines:\n",
    "    eng_vocab = eng_vocab.union(curr_list)\n",
    "\n",
    "    \n",
    "print(\"Set of symbols in Russian sentences\")\n",
    "print(rus_vocab)\n",
    "print(len(rus_vocab))\n",
    "print()\n",
    "print(\"---\")\n",
    "print()\n",
    "print(\"Set of symbols in English sentences\")\n",
    "print(eng_vocab)\n",
    "print(len(eng_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus_tokenizer = LangTokenizer(vocab=rus_vocab)\n",
    "eng_tokenizer = LangTokenizer(vocab=eng_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adopt them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d6cf812a5ca4fd78c91482c332346d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rus_tokenizer.adopt(rus_lines, ad_num=True, num_cycles=2, part_add = 0.50, verbose_main = True, verbose_NGW = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abf6dfdcc9da4ab3a5335f9a7c4d2235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eng_tokenizer.adopt(eng_lines, ad_num=True, num_cycles=2, part_add = 0.50, verbose_main = True, verbose_NGW = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check most frequently occurring gramms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num:  37580 gramm:  o\n",
      "num:  34670 gramm:  a\n",
      "num:  31676 gramm:  e\n",
      "num:  31504 gramm:  d\n",
      "num:  30855 gramm:  i\n",
      "num:  30491 gramm:  e \n",
      "num:  28887 gramm:  r\n",
      "num:  28692 gramm:  s\n",
      "num:  26357 gramm:  t \n",
      "num:  25924 gramm:  w\n",
      "num:  25215 gramm:  h\n",
      "num:  24496 gramm:   \n",
      "num:  24270 gramm:  c\n",
      "num:  23904 gramm:  m\n",
      "num:  23623 gramm:  k\n",
      "num:  23592 gramm:  p\n",
      "num:  22774 gramm:  l\n",
      "num:  21864 gramm:  g\n",
      "num:  21602 gramm:  t\n",
      "num:  21126 gramm:  u\n",
      "num:  20142 gramm:  n\n",
      "num:  19610 gramm:  y \n",
      "num:  18564 gramm:  d \n",
      "num:  18334 gramm:  f\n",
      "num:  16882 gramm:  i \n",
      "num:  16629 gramm:  you\n",
      "num:  15743 gramm:  b\n",
      "num:  15720 gramm:  re\n",
      "num:  15255 gramm:  er\n",
      "num:  15191 gramm:  tom \n",
      "num:  15061 gramm:   t\n",
      "num:  13899 gramm:  s \n",
      "num:  13864 gramm:  th\n",
      "num:  12798 gramm:  y\n",
      "num:  12645 gramm:  an\n",
      "num:  12541 gramm:  on\n",
      "num:  11682 gramm:   w\n",
      "num:  11517 gramm:  in\n",
      "num:  11192 gramm:  he\n",
      "num:  11181 gramm:  ing\n"
     ]
    }
   ],
   "source": [
    "eng_tokenizer.print_top(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num:  38464 gramm:  .\n",
      "num:  32425 gramm:  с\n",
      "num:  29887 gramm:  у\n",
      "num:  29142 gramm:  и\n",
      "num:  29102 gramm:   \n",
      "num:  26954 gramm:  т\n",
      "num:  24985 gramm:  е\n",
      "num:  22469 gramm:  я \n",
      "num:  19711 gramm:  а\n",
      "num:  19080 gramm:  н\n",
      "num:  18846 gramm:  м\n",
      "num:  18510 gramm:  ы\n",
      "num:  17875 gramm:  не\n",
      "num:  17850 gramm:  л\n",
      "num:  17752 gramm:  б\n",
      "num:  16634 gramm:  ж\n",
      "num:  16598 gramm:  к\n",
      "num:  16530 gramm:  й\n",
      "num:  16422 gramm:  ш\n",
      "num:  16122 gramm:  в\n",
      "num:  15736 gramm:  на\n",
      "num:  15507 gramm:  з\n",
      "num:  14496 gramm:  том \n",
      "num:  14308 gramm:  ра\n",
      "num:  14237 gramm:  д\n",
      "num:  13781 gramm:  по\n",
      "num:  13023 gramm:  г\n",
      "num:  12989 gramm:   с\n",
      "num:  12718 gramm:  е \n",
      "num:  12696 gramm:  ?\n",
      "num:  12618 gramm:  , \n",
      "num:  12496 gramm:  ю\n",
      "num:  12363 gramm:  ст\n",
      "num:  12125 gramm:  но\n",
      "num:  11580 gramm:  ь \n",
      "num:  11546 gramm:  о \n",
      "num:  11506 gramm:   в\n",
      "num:  10983 gramm:  ч\n",
      "num:  10922 gramm:  о\n",
      "num:  10892 gramm:  ко\n"
     ]
    }
   ],
   "source": [
    "rus_tokenizer.print_top(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization and detokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 165, 165, 108, 10, 186, 71, 113, 29, 46, 2]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokened = rus_tokenizer.tokenize(\"мама мыла раму\")\n",
    "tokened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'мама мыла раму'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rus_tokenizer.detokenize(tokened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
